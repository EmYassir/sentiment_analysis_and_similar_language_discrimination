{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DSL_test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lYC9yMajmxkc","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import logging\n","from keras import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import Adam\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n","from sklearn.metrics import classification_report\n","\n","from dictionary import Dictionary\n","from text_util import Text_Util\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","tfidf = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSu5fxW9qE8D","colab_type":"code","outputId":"023047d3-cba2-4fa9-d81a-7d228a2a403e","executionInfo":{"status":"ok","timestamp":1575141766935,"user_tz":300,"elapsed":5223,"user":{"displayName":"Alexander Bustamante","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCfk75rvS8GtGh5i552oKU07h41ILuzN8FOdnBZEw=s64","userId":"17926823510905786271"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# Get dsl train, validation, and test sets and vectorize them\n","# es-AR es-ES es-PE fr-CA fr-FR\n","dsl_train = pd.read_csv('./DSL-TRAIN.txt', sep='\\t', names=['sentence', 'c'])\n","# dsl_train.query('c == \"es-AR\" or c == \"es-ES\" or c == \"es-PE\" or c == \"fr-FR\" or c == \"fr-CA\"', inplace=True)\n","# dsl_train.query('c == \"es-AR\" or c == \"es-ES\" or c == \"es-PE\"', inplace=True)\n","dsl_train.query('c == \"fr-FR\" or c == \"fr-CA\"', inplace=True)\n","dsl_x_train = dsl_train.sentence.values\n","dsl_y_train = dsl_train.c.values\n","dsl_test = pd.read_csv('./DSL-DEV.txt', sep='\\t', names=['sentence', 'c'])\n","# dsl_test.query('c == \"es-AR\" or c == \"es-ES\" or c == \"es-PE\" or c == \"fr-FR\" or c == \"fr-CA\"', inplace=True)\n","# dsl_test.query('c == \"es-AR\" or c == \"es-ES\" or c == \"es-PE\"', inplace=True)\n","dsl_test.query('c == \"fr-FR\" or c == \"fr-CA\"', inplace=True)\n","dsl_x_test = dsl_test.sentence.values\n","dsl_y_test = dsl_test.c.values\n","le = LabelEncoder()\n","dsl_y_train = le.fit_transform(dsl_y_train)\n","dsl_y_test = le.transform(dsl_y_test)\n","# *********************************************************************************************\n","logger.info('### Creating dictionary...')\n","sorted_labels = np.unique(dsl_y_train)\n","dic = Dictionary(sorted_labels)\n","# Preprocessing text\n","logger.info('### Preprocessing text...')\n","text_util = Text_Util()\n","dsl_x_train = text_util.get_preprocessed_tokenized_sentences_dsl(dsl_x_train)\n","if tfidf:\n","  # Updating dictionary\n","  logger.info('### Updating dictionary...')\n","  for i in range(len(dsl_x_train)):\n","      dic.update_tokenized(dsl_x_train[i], dsl_y_train[i])\n","  selected_words = {}\n","  for i, l in enumerate(sorted_labels):\n","      u_list = dic.get_n_words_unique_to_label(l, 1000)\n","      o_list = dic.get_n_top_words_given_label(l, 5000)\n","      for u in u_list:\n","          selected_words[u] = True\n","\n","      for o in o_list:\n","          selected_words[o] = True\n","\n","  print(f\"Size of selected words set {len(selected_words)}\")\n","  x = []\n","\n","  for tokenized_comment in dsl_x_train:\n","      x.append(np.array([w for w in tokenized_comment if w in selected_words]))\n","  dsl_x_train = x\n","  dsl_x_train = list(map(\" \".join, dsl_x_train))\n","  # *********************************************************************************************\n","  vectorizer = TfidfVectorizer()\n","  dsl_x_train = vectorizer.fit_transform(dsl_x_train)\n","  dsl_x_test = vectorizer.transform(dsl_x_test)\n","  dsl_x_train, dsl_x_val, dsl_y_train, dsl_y_val = \\\n","      train_test_split(dsl_x_train, dsl_y_train, test_size=.2, shuffle=True, stratify=dsl_y_train)\n","  logger.info(f'### dsl_x_train.shape {dsl_x_train.shape}')\n","  logger.info(f'### dsl_x_train.shape {dsl_x_val.shape}')\n","\n","dsl_y_train = [item for item in dsl_y_train.astype(str)]\n","dsl_y_val = [item for item in dsl_y_val.astype(str)]\n","# dsl_y_test = [item for item in dsl_y_test.astype(str)]\n","binarizer = MultiLabelBinarizer()\n","dsl_y_train = binarizer.fit_transform(dsl_y_train)\n","dsl_y_val = binarizer.transform(dsl_y_val)\n","# dsl_y_test = binarizer.transform(dsl_y_test)\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["INFO:__main__:### Creating dictionary...\n","INFO:__main__:### Preprocessing text...\n","INFO:__main__:### Updating dictionary...\n"],"name":"stderr"},{"output_type":"stream","text":["Size of selected words set 7945\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:__main__:### dsl_x_train.shape (28137, 7920)\n","INFO:__main__:### dsl_x_train.shape (7035, 7920)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8DBXDn9XdKjj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":330},"outputId":"0cdfd277-b092-4117-dcf8-7dc0781093f9","executionInfo":{"status":"ok","timestamp":1575141919622,"user_tz":300,"elapsed":18949,"user":{"displayName":"Alexander Bustamante","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCfk75rvS8GtGh5i552oKU07h41ILuzN8FOdnBZEw=s64","userId":"17926823510905786271"}}},"source":["if tfidf:\n","  # Train model\n","  classifier = Sequential()\n","  # First Hidden Layer\n","  classifier.add(Dense(32, activation='relu', kernel_initializer='random_normal', input_dim=dsl_x_train.shape[1]))\n","  # Dropout\n","  classifier.add(Dropout(rate=0.1))\n","  # # Second Hidden Layer\n","  # classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n","  # # Dropout\n","  # classifier.add(Dropout(rate=0.1))\n","  # Output Layer\n","  classifier.add(Dense(2, activation='softmax', kernel_initializer='random_normal'))\n","  opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n","  classifier.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","  # Fitting the data to the training dataset\n","  classifier.fit(dsl_x_train, dsl_y_train, batch_size=64, epochs=4, validation_data=(dsl_x_val, dsl_y_val))\n","  # metrics = classifier.evaluate(dsl_x_test, dsl_y_test, verbose=1)\n","  # for i in range(len(classifier.metrics_names)):\n","  #     logger.info(f'{classifier.metrics_names[i]}: {metrics[i]}')\n","\n","dsl_y_pred = classifier.predict(dsl_x_test)\n","dsl_y_pred = np.argmax(dsl_y_pred, axis=1)\n","print(classification_report(le.inverse_transform(dsl_y_test), le.inverse_transform(dsl_y_pred)))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Train on 28137 samples, validate on 7035 samples\n","Epoch 1/4\n","28137/28137 [==============================] - 6s 204us/step - loss: 0.4721 - acc: 0.8079 - val_loss: 0.2723 - val_acc: 0.8928\n","Epoch 2/4\n","28137/28137 [==============================] - 4s 125us/step - loss: 0.2166 - acc: 0.9165 - val_loss: 0.2291 - val_acc: 0.9042\n","Epoch 3/4\n","28137/28137 [==============================] - 4s 127us/step - loss: 0.1632 - acc: 0.9365 - val_loss: 0.2289 - val_acc: 0.9046\n","Epoch 4/4\n","28137/28137 [==============================] - 4s 129us/step - loss: 0.1361 - acc: 0.9480 - val_loss: 0.2370 - val_acc: 0.9029\n","              precision    recall  f1-score   support\n","\n","       fr-CA       0.90      0.86      0.88      2000\n","       fr-FR       0.87      0.90      0.88      1990\n","\n","    accuracy                           0.88      3990\n","   macro avg       0.88      0.88      0.88      3990\n","weighted avg       0.88      0.88      0.88      3990\n","\n"],"name":"stdout"}]}]}