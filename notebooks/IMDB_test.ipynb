{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMDB_test.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0VslN0Cp8_yH","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import logging\n","from keras import Sequential\n","from keras.layers import Dense, Dropout, LSTM , Embedding, Bidirectional, GlobalMaxPool1D\n","from keras.optimizers import Adam, SGD\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","from dictionary import Dictionary\n","from text_util import Text_Util\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","tfidf = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Fab-ABN9wR7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"4c485782-12fc-4a83-c18a-74f87144c1b1","executionInfo":{"status":"ok","timestamp":1575133325259,"user_tz":300,"elapsed":76892,"user":{"displayName":"Alexander Bustamante","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCfk75rvS8GtGh5i552oKU07h41ILuzN8FOdnBZEw=s64","userId":"17926823510905786271"}}},"source":["# Get IMDB train, validation, and test sets and vectorize them\n","imdb_train = pd.read_csv('./train/pos.csv', names=['review', 'c'])\n","imdb_train = imdb_train.append(pd.read_csv('./train/neg.csv', names=['review', 'c']))\n","imdb_x_train = imdb_train.review.values\n","imdb_y_train = imdb_train.c.values\n","imdb_test = pd.read_csv('./test/pos.csv', names=['review', 'c'])\n","imdb_test = imdb_test.append(pd.read_csv('./test/neg.csv', names=['review', 'c']))\n","imdb_x_test = imdb_test.review.values\n","imdb_y_test = imdb_test.c.values\n","\n","# *********************************************************************************************\n","logger.info('### Creating dictionary...')\n","sorted_labels = np.unique(imdb_y_train)\n","dic = Dictionary(sorted_labels)\n","# Preprocessing text\n","logger.info('### Preprocessing text...')\n","text_util = Text_Util()\n","imdb_x_train = text_util.get_preprocessed_tokenized_sentences(imdb_x_train)\n","if tfidf:\n","  # Updating dictionary\n","  logger.info('### Updating dictionary...')\n","  for i in range(len(imdb_x_train)):\n","      dic.update_tokenized(imdb_x_train[i], imdb_y_train[i])\n","  selected_words = {}\n","  for i, l in enumerate(sorted_labels):\n","      u_list = dic.get_n_words_unique_to_label(l, 1000)\n","      o_list = dic.get_n_top_words_given_label(l, 5000)\n","      for u in u_list:\n","          selected_words[u] = True\n","\n","      for o in o_list:\n","          selected_words[o] = True\n","\n","  print(f\"Size of selected words set {len(selected_words)}\")\n","  x = []\n","\n","  for tokenized_comment in imdb_x_train:\n","      x.append(np.array([w for w in tokenized_comment if w in selected_words]))\n","  imdb_x_train = x\n","  imdb_x_train = list(map(\" \".join, imdb_x_train))\n","  # *********************************************************************************************\n","\n","  vectorizer = TfidfVectorizer()\n","  imdb_x_train = vectorizer.fit_transform(imdb_x_train)\n","  imdb_x_test = text_util.get_preprocessed_tokenized_sentences(imdb_x_test)\n","  imdb_x_test = list(map(\" \".join, imdb_x_test))\n","  imdb_x_test = vectorizer.transform(imdb_x_test)\n","  imdb_x_train, imdb_x_val, imdb_y_train, imdb_y_val = \\\n","      train_test_split(imdb_x_train, imdb_y_train, test_size=.2, shuffle=True, stratify=imdb_y_train)\n","  logger.info(f'### imdb_x_train.shape {imdb_x_train.shape}')\n","  logger.info(f'### imdb_x_val.shape {imdb_x_val.shape}')\n","else:\n","  logger.info('### Preprocessing for word2vec embeddings...')\n","  imdb_x_train = list(map(\" \".join, imdb_x_train))\n","  imdb_x_test = text_util.get_preprocessed_tokenized_sentences(imdb_x_test)\n","  imdb_x_test = list(map(\" \".join, imdb_x_test))\n","  max_words = 6000\n","  tokenizer = Tokenizer(num_words=max_words)\n","  tokenizer.fit_on_texts(imdb_x_train)\n","  imdb_x_train = tokenizer.texts_to_sequences(imdb_x_train)\n","  imdb_x_test = tokenizer.texts_to_sequences(imdb_x_test)\n","  max_len = 130\n","  imdb_x_train = pad_sequences(imdb_x_train, maxlen=max_len)\n","  imdb_x_test = pad_sequences(imdb_x_test, maxlen=max_len)\n","  imdb_x_train, imdb_x_val, imdb_y_train, imdb_y_val = \\\n","      train_test_split(imdb_x_train, imdb_y_train, test_size=.2, shuffle=True, stratify=imdb_y_train)\n","  logger.info(f'### imdb_x_train.shape {imdb_x_train.shape}')\n","  logger.info(f'### imdb_x_val.shape {imdb_x_val.shape}')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["INFO:__main__:### Creating dictionary...\n","INFO:__main__:### Preprocessing text...\n","INFO:__main__:### Preprocessing for word2vec embeddings...\n","INFO:__main__:### imdb_x_train.shape (20000, 130)\n","INFO:__main__:### imdb_x_val.shape (5000, 130)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LsAqEFZy95Qo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"outputId":"865d25c1-3be3-4dcb-c5f8-69d44b3d813c","executionInfo":{"status":"ok","timestamp":1575134359510,"user_tz":300,"elapsed":163466,"user":{"displayName":"Alexander Bustamante","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCfk75rvS8GtGh5i552oKU07h41ILuzN8FOdnBZEw=s64","userId":"17926823510905786271"}}},"source":["if tfidf:\n","  # Train model\n","  classifier = Sequential()\n","  # First Hidden Layer\n","  classifier.add(Dense(32, activation='relu', kernel_initializer='random_normal', input_dim=imdb_x_train.shape[1]))\n","  # Dropout\n","  classifier.add(Dropout(rate=0.1))\n","  # Second Hidden Layer\n","  classifier.add(Dense(64, activation='relu', kernel_initializer='random_normal'))\n","  # Dropout\n","  classifier.add(Dropout(rate=0.1))\n","  # Output Layer\n","  classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n","  opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n","  # opt = SGD(lr=0.01, momentum=0.0, nesterov=False)\n","  classifier.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","  # Fitting the data to the training dataset\n","  classifier.fit(imdb_x_train, imdb_y_train, batch_size=64, epochs=4, validation_data=(imdb_x_val, imdb_y_val))\n","else:\n","  embed_size = 128\n","  classifier = Sequential()\n","  classifier.add(Embedding(max_words, embed_size))\n","  classifier.add(Bidirectional(LSTM(32, return_sequences = True)))\n","  classifier.add(GlobalMaxPool1D())\n","  classifier.add(Dense(20, activation=\"relu\"))\n","  classifier.add(Dropout(0.05))\n","  classifier.add(Dense(1, activation=\"sigmoid\"))\n","  opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n","  classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  batch_size = 64\n","  epochs = 1\n","  classifier.fit(imdb_x_train, imdb_y_train, batch_size=batch_size, epochs=epochs, validation_data=(imdb_x_val, imdb_y_val))\n","\n","imdb_y_pred = classifier.predict(imdb_x_test)\n","imdb_y_pred = imdb_y_pred > 0.5\n","print(classification_report(imdb_y_test, imdb_y_pred))"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Train on 20000 samples, validate on 5000 samples\n","Epoch 1/1\n","20000/20000 [==============================] - 112s 6ms/step - loss: 0.4007 - acc: 0.8193 - val_loss: 0.3259 - val_acc: 0.8676\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.91      0.87     12500\n","           1       0.90      0.82      0.86     12500\n","\n","    accuracy                           0.86     25000\n","   macro avg       0.87      0.86      0.86     25000\n","weighted avg       0.87      0.86      0.86     25000\n","\n"],"name":"stdout"}]}]}